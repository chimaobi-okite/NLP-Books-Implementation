## Pretraining Roberta From Scratch

This folder implements the pretraining task of a RoBerta type model as described in chapter 3 of Transformers with NLP book.

Transformers usually follow a two step process

* pretraining and
* finetuning.

For most tasks, we would usually load a pretrained model then finetune it for our use case.

Here we explore ways to pretrain our transform from scratch for a Masked_Language_modelling task based on Immanuel Kant Works.

